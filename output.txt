WARNING:root:float16 models may not work on CPU. Consider using a GPU or bfloat16.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:01,  2.83it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:01,  2.35it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:01<00:01,  2.16it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:01<00:00,  2.14it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:02<00:00,  2.17it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:02<00:00,  2.37it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:02<00:00,  2.30it/s]
WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.
WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping
Loaded pretrained model GSAI-ML/LLaDA-8B-Base into HookedTransformer
HookedTransformerConfig:
{'NTK_by_parts_factor': 8.0,
 'NTK_by_parts_high_freq_factor': 4.0,
 'NTK_by_parts_low_freq_factor': 1.0,
 'NTK_original_ctx_len': 8192,
 'act_fn': 'silu',
 'attention_dir': 'bidirectional',
 'attn_only': False,
 'attn_scale': 11.313708498984761,
 'attn_scores_soft_cap': -1.0,
 'attn_types': None,
 'checkpoint_index': None,
 'checkpoint_label_type': None,
 'checkpoint_value': None,
 'd_head': 128,
 'd_mlp': 12288,
 'd_model': 4096,
 'd_vocab': 126464,
 'd_vocab_out': 126464,
 'decoder_start_token_id': None,
 'default_prepend_bos': True,
 'device': device(type='cuda'),
 'dtype': torch.float16,
 'eps': 1e-05,
 'experts_per_token': None,
 'final_rms': True,
 'from_checkpoint': False,
 'gated_mlp': True,
 'init_mode': 'gpt2',
 'init_weights': False,
 'initializer_range': 0.0125,
 'load_in_4bit': False,
 'model_name': 'LLaDA-8B-Base',
 'n_ctx': 8192,
 'n_devices': 1,
 'n_heads': 32,
 'n_key_value_heads': 32,
 'n_layers': 32,
 'n_params': 6979321856,
 'normalization_type': 'RMSPre',
 'num_experts': None,
 'original_architecture': 'LLaDAModelLM',
 'output_logits_soft_cap': -1.0,
 'parallel_attn_mlp': False,
 'positional_embedding_type': 'rotary',
 'post_embedding_ln': False,
 'relative_attention_max_distance': None,
 'relative_attention_num_buckets': None,
 'rotary_adjacent_pairs': False,
 'rotary_base': 500000.0,
 'rotary_dim': 128,
 'scale_attn_by_inverse_layer_idx': False,
 'seed': None,
 'tie_word_embeddings': False,
 'tokenizer_name': 'GSAI-ML/LLaDA-8B-Base',
 'tokenizer_prepends_bos': False,
 'trust_remote_code': True,
 'ungroup_grouped_query_attention': False,
 'use_NTK_by_parts_rope': False,
 'use_attn_in': False,
 'use_attn_result': False,
 'use_attn_scale': True,
 'use_hook_mlp_in': False,
 'use_hook_tokens': False,
 'use_local_attn': False,
 'use_normalization_before_and_after': False,
 'use_qk_norm': False,
 'use_split_qkv_input': False,
 'window_size': None}
Starting Generation: 'The capital of France is' + 10 masks
Step 0/50: <|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|>
Step 10/50: <|mdm_mask|><|mdm_mask|><|mdm_mask|>,<|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|>,<|mdm_mask|>
Step 20/50: ,<|mdm_mask|>,,<|mdm_mask|>,<|mdm_mask|><|mdm_mask|><|mdm_mask|><|mdm_mask|>
Step 30/50: ,<|mdm_mask|>,,<|mdm_mask|>,<|mdm_mask|>,<|mdm_mask|> sums
Step 40/50: ,<|mdm_mask|> set,,,,,<|mdm_mask|>,

Final Output: <|startoftext|>The capital of France is,,,,,,,,,,
