HookedTransformerConfig:
{'NTK_by_parts_factor': 8.0,
 'NTK_by_parts_high_freq_factor': 4.0,
 'NTK_by_parts_low_freq_factor': 1.0,
 'NTK_original_ctx_len': 8192,
 'act_fn': 'silu',
 'attention_dir': 'bidirectional',
 'attn_only': False,
 'attn_scale': 11.313708498984761,
 'attn_scores_soft_cap': -1.0,
 'attn_types': None,
 'checkpoint_index': None,
 'checkpoint_label_type': None,
 'checkpoint_value': None,
 'd_head': 128,
 'd_mlp': 12288,
 'd_model': 4096,
 'd_vocab': 126464,
 'd_vocab_out': 126464,
 'decoder_start_token_id': None,
 'default_prepend_bos': True,
 'device': device(type='cuda'),
 'dtype': torch.float32,
 'eps': 1e-05,
 'experts_per_token': None,
 'final_rms': True,
 'from_checkpoint': False,
 'gated_mlp': True,
 'init_mode': 'gpt2',
 'init_weights': False,
 'initializer_range': 0.0125,
 'load_in_4bit': False,
 'model_name': 'LLaDA-8B-Base',
 'n_ctx': 8192,
 'n_devices': 1,
 'n_heads': 32,
 'n_key_value_heads': 8,
 'n_layers': 32,
 'n_params': 6979321856,
 'normalization_type': 'RMSPre',
 'num_experts': None,
 'original_architecture': 'LLaDAModelLM',
 'output_logits_soft_cap': -1.0,
 'parallel_attn_mlp': False,
 'positional_embedding_type': 'rotary',
 'post_embedding_ln': False,
 'relative_attention_max_distance': None,
 'relative_attention_num_buckets': None,
 'rotary_adjacent_pairs': False,
 'rotary_base': 500000.0,
 'rotary_dim': 128,
 'scale_attn_by_inverse_layer_idx': False,
 'seed': None,
 'tie_word_embeddings': False,
 'tokenizer_name': 'GSAI-ML/LLaDA-8B-Base',
 'tokenizer_prepends_bos': None,
 'trust_remote_code': True,
 'ungroup_grouped_query_attention': False,
 'use_NTK_by_parts_rope': False,
 'use_attn_in': False,
 'use_attn_result': False,
 'use_attn_scale': True,
 'use_hook_mlp_in': False,
 'use_hook_tokens': False,
 'use_local_attn': False,
 'use_normalization_before_and_after': False,
 'use_qk_norm': False,
 'use_split_qkv_input': False,
 'window_size': None}
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:02,  2.28it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:02,  2.00it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:01<00:01,  1.87it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:02<00:01,  1.84it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:02<00:00,  1.83it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:03<00:00,  2.01it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:03<00:00,  1.95it/s]
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.29.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.25.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.12.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.6.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.23.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.8.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.1.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.17.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.11.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.26.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.3.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.18.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.30.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.22.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.10.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.15.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.28.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.20.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.14.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.27.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.31.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.0.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.30.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.6.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.8.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.13.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.28.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.22.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.21.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.4.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.17.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.27.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.0.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.10.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.5.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.19.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.23.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.18.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.9.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.31.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.16.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.11.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.2.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.19.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.15.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.21.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.9.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.14.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.24.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.1.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.24.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.25.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.3.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.2.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.26.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.7.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.4.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.20.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.16.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.5.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.29.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.7.attn._W_K
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.12.attn._W_V
WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.13.attn._W_V
WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping
Traceback (most recent call last):
  File "/workspace/TransformerLens/llada_lens.py", line 36, in <module>
    tl_model = HookedTransformer.from_pretrained(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/TransformerLens/transformer_lens/HookedTransformer.py", line 1394, in from_pretrained
    model.move_model_modules_to_device()
  File "/workspace/TransformerLens/transformer_lens/HookedTransformer.py", line 1115, in move_model_modules_to_device
    block.to(devices.get_best_available_device(self.cfg))
  File "/venv/llada_lens/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/venv/llada_lens/lib/python3.12/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/venv/llada_lens/lib/python3.12/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/venv/llada_lens/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 23.56 GiB of which 76.00 MiB is free. Including non-PyTorch memory, this process has 23.48 GiB memory in use. Of the allocated memory 23.21 GiB is allocated by PyTorch, and 13.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
